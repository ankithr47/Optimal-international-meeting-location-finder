{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "7c00bd5c-5c3b-484c-8c52-c5dfe3e3038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: polars in /opt/anaconda3/lib/python3.13/site-packages (1.35.2)\n",
      "Requirement already satisfied: polars-runtime-32==1.35.2 in /opt/anaconda3/lib/python3.13/site-packages (from polars) (1.35.2)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install polars \n",
    "!pip install pandas\n",
    "\n",
    "import polars as pl\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "37d15c14-b621-4e8d-9ca7-d5b0e8a0478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WE WOULD NEED A JSON_FILE AS AN INPUT\n",
    "'''\n",
    "json_file='/Users/ankithrangan/Desktop/ChallengeData/seen_files/input_2.json'\n",
    "\n",
    "\n",
    "#selecting attendees from json file\n",
    "with open(json_file,'r') as file:\n",
    "    data=json.load(file)\n",
    "attendees=data['attendees']\n",
    "\n",
    "\n",
    "\n",
    "#example attendees\n",
    "'''\n",
    "\n",
    "attendees={\n",
    "\t\t\"Mumbai\": 1,\n",
    "        \"London\": 1,\n",
    "\t\t\"Shanghai\": 0,\n",
    "\t\t\"Hong Kong\": 1,\n",
    "\t\t\"Singapore\": 0,\n",
    "\t\t\"Sydney\": 1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "8b917389-940b-4a06-bbd7-104f3e3bcc71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\na = 5\\n\\n# Open and read the JSON file\\ndef file_list(json_file):\\n    # Read the JSON file\\n    with open(json_file, \"r\") as file:\\n        data = json.load(file)\\n    start_date_str = data[\"availability_window\"][\"start\"]\\n    end_date_str = data[\"availability_window\"][\"end\"]\\n\\n    start_date = datetime.datetime.strptime(start_date_str.replace(\"Z\", \"\"), \"%Y-%m-%dT%H:%M:%S\")\\n    end_date = datetime.datetime.strptime(end_date_str.replace(\"Z\", \"\"), \"%Y-%m-%dT%H:%M:%S\")\\n\\n    files = []\\n\\n    # Loop from start to end date within availability window\\n    current_date = start_date - datetime.timedelta(days=a)\\n    while current_date.date() <= end_date.date():\\n        # For each file, construct path like ChallengeData/schedules/2025/12/10.csv\\n        file_path = os.path.join(\\n            \"schedules\",\\n            str(current_date.year),\\n            f\"{current_date.month:02d}\",\\n            f\"{current_date.day:02d}.csv\"\\n        )\\n        files.append(file_path)\\n        current_date += datetime.timedelta(days=1)\\n\\n    return files\\n\\n# example path: /Users/ankithrangan/Desktop/ChallengeData/schedules/2023/05/04.csv\\n#/Users/ankithrangan/Desktop/ChallengeData/schedules/2025/03/03.csv\\nschedule_files=[file_list(json_file)] #list of csv files\\nnew_files = schedule_files[0]\\nnew_files\\n'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "'''\n",
    "a = 5\n",
    "\n",
    "# Open and read the JSON file\n",
    "def file_list(json_file):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    start_date_str = data[\"availability_window\"][\"start\"]\n",
    "    end_date_str = data[\"availability_window\"][\"end\"]\n",
    "\n",
    "    start_date = datetime.datetime.strptime(start_date_str.replace(\"Z\", \"\"), \"%Y-%m-%dT%H:%M:%S\")\n",
    "    end_date = datetime.datetime.strptime(end_date_str.replace(\"Z\", \"\"), \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    files = []\n",
    "\n",
    "    # Loop from start to end date within availability window\n",
    "    current_date = start_date - datetime.timedelta(days=a)\n",
    "    while current_date.date() <= end_date.date():\n",
    "        # For each file, construct path like ChallengeData/schedules/2025/12/10.csv\n",
    "        file_path = os.path.join(\n",
    "            \"schedules\",\n",
    "            str(current_date.year),\n",
    "            f\"{current_date.month:02d}\",\n",
    "            f\"{current_date.day:02d}.csv\"\n",
    "        )\n",
    "        files.append(file_path)\n",
    "        current_date += datetime.timedelta(days=1)\n",
    "\n",
    "    return files\n",
    "\n",
    "# example path: /Users/ankithrangan/Desktop/ChallengeData/schedules/2023/05/04.csv\n",
    "#/Users/ankithrangan/Desktop/ChallengeData/schedules/2025/03/03.csv\n",
    "schedule_files=[file_list(json_file)] #list of csv files\n",
    "new_files = schedule_files[0]\n",
    "new_files\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "2f33a8bc-4593-46c3-80e2-bc2d8bbb6ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(new_files)):\\n    new_files[i] = f\"/Users/ankithrangan/Desktop/ChallengeData/{new_files[i]}\"\\nnew_files\\n'"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in range(len(new_files)):\n",
    "    new_files[i] = f\"/Users/ankithrangan/Desktop/ChallengeData/{new_files[i]}\"\n",
    "new_files\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "dc98221f-2eb8-4bcb-adca-ac269aa51817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nschedules=pl.scan_csv(new_files, infer_schema_length=10000).collect() #list of paths to access csv files (within availability)\\nschedules\\n#/Users/ankithrangan/Desktop/ChallengeData/schedules/2023/05/04.csv\\n'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#############################################\n",
    "\"\"\" #############################################\n",
    "from pathlib import Path\n",
    "# If your file_list() returned relative paths like \"schedules/2025/12/05.csv\"\n",
    "base = Path(\"/Users/ankithrangan/Desktop/ChallengeData\")\n",
    "\n",
    "# If schedule_files was [[...]], flatten it first:\n",
    "flat_files = schedule_files[0] if isinstance(schedule_files[0], list) else schedule_files\n",
    "\n",
    "# Build absolute paths and check existence\n",
    "abs_paths = [base / Path(p.strip()) for p in flat_files]\n",
    "existing = [str(p) for p in abs_paths if p.is_file()]\n",
    "missing  = [str(p) for p in abs_paths if not p.is_file()]\n",
    "\n",
    "if missing:\n",
    "    print(f\"Missing {len(missing)} files. First few:\\n\", \"\\n\".join(missing[:5]))\n",
    "\n",
    "if not existing:\n",
    "    raise FileNotFoundError(\"No existing CSVs to read after filtering.\")\n",
    "\n",
    "# Combine them into a single DataFrame\n",
    "schedules = pl.scan_csv(existing).collect()\n",
    "\n",
    "\"\"\"#############################################\n",
    "\n",
    "'''\n",
    "schedules=pl.scan_csv(new_files, infer_schema_length=10000).collect() #list of paths to access csv files (within availability)\n",
    "schedules\n",
    "#/Users/ankithrangan/Desktop/ChallengeData/schedules/2023/05/04.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "27d3f638-328e-45a9-9f07-444b0a5430df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "targets = [\n",
    "    # === Asia ===\n",
    "    \"HND\", \"NRT\", \"PEK\", \"PKX\", \"PVG\", \"SHA\", \"CAN\", \"HKG\", \"ICN\", \"GMP\",\n",
    "    \"SIN\", \"BKK\", \"DMK\", \"KUL\", \"CGK\", \"MNL\", \"DEL\", \"BOM\", \"BLR\", \"MAA\",\n",
    "    \"HYD\", \"DOH\", \"DXB\", \"AUH\", \"BAH\", \"KWI\", \"MCT\", \"RUH\", \"JED\", \"IKA\",\n",
    "    \"AMM\", \"BEY\", \"TLV\", \"HKT\", \"SGN\", \"HAN\", \"DAD\", \"KIX\", \"ITM\", \"CTS\",\n",
    "    \"FUK\", \"KHH\", \"TPE\", \"BNE\", \"PER\", \"MEL\", \"ADL\", \"AKL\", \"WLG\", \"CHC\",\n",
    "    \"DPS\", \"CEB\", \"GUM\", \"SPN\",\n",
    "\n",
    "    # === Europe ===\n",
    "    \"LHR\", \"LGW\", \"MAN\", \"STN\", \"LTN\", \"EDI\", \"BHX\", \"GLA\", \"DUB\", \"CDG\",\n",
    "    \"ORY\", \"AMS\", \"FRA\", \"MUC\", \"BER\", \"DUS\", \"ZRH\", \"GVA\", \"VIE\", \"BRU\",\n",
    "    \"CPH\", \"OSL\", \"ARN\", \"HEL\", \"WAW\", \"KRK\", \"PRG\", \"BUD\", \"ZAG\", \"LIS\",\n",
    "    \"OPO\", \"MAD\", \"BCN\", \"PMI\", \"FCO\", \"MXP\", \"LIN\", \"ATH\", \"SKG\", \"IST\",\n",
    "    \"SAW\", \"RIX\", \"TLL\", \"VNO\", \"KEF\", \"OTP\", \"BEG\", \"SOF\", \"LCA\", \"MLA\",\n",
    "    \"SVO\", \"DME\", \"LED\", \"KBP\", \"ODS\",\n",
    "\n",
    "    # === Australia / Oceania ===\n",
    "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"ADL\", \"CBR\", \"HBA\", \"DRW\", \"OOL\", \"AKL\",\n",
    "    \"WLG\", \"CHC\", \"NAN\", \"PPT\", \"APW\", \"SPN\", \"GUM\", \"HIR\", \"NOUMEA\", \"TBU\",\n",
    "    \"POM\"\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "df = pl.read_csv(\"12.csv\", ignore_errors=True)\n",
    "\n",
    "\n",
    "\n",
    "df = df.with_columns([\n",
    "    pl.col(\"SCHEDULED_DEPARTURE_DATE_TIME_UTC\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.3f\", strict=False).alias(\"UTC_DEP\"),\n",
    "    pl.col(\"SCHEDULED_DEPARTURE_DATE_TIME_LOCAL\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.3f\", strict=False).alias(\"LCL_DEP\"),\n",
    "    pl.col(\"SCHEDULED_ARRIVAL_DATE_TIME_UTC\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.3f\", strict=False).alias(\"UTC_ARR\"),\n",
    "    pl.col(\"SCHEDULED_ARRIVAL_DATE_TIME_LOCAL\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.3f\", strict=False).alias(\"LCL_ARR\")\n",
    "])\n",
    "\n",
    "\n",
    "df = df[:,[\"DEPAPT\",       #1\n",
    "           \"DEPCITY\",      #2\n",
    "           \"ARRAPT\",       #3\n",
    "           \"ARRCITY\",      #4\n",
    "           \"UTC_DEP\",      #5\n",
    "           \"LCL_DEP\",      #6\n",
    "           \"UTC_ARR\",      #7\n",
    "           \"LCL_ARR\",      #8\n",
    "           \"TOTAL_SEATS\",  #9\n",
    "           \"OAG_SCHEDULE_FINGERPRINT\" #10\n",
    "          ]]\n",
    "\n",
    "df = df.with_columns([(pl.col(\"UTC_ARR\") - pl.col(\"LCL_ARR\")).alias(\"LCL2UTC\")])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "673750aa-ce9a-460f-b6d1-5800f67283ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOM', 'LON', 'HKG', 'SYD']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfiltered_schedules = schedules.filter(filter_expression) #gives all rows with airports mentioned in the input\\nfiltered_schedules\\n'"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "airport_dictionary = {\n",
    "    'Mumbai': ['BOM'],\n",
    "    'London': ['LHR', 'STN', 'LGW', 'LTN'],\n",
    "    'Paris': ['CDG', 'BVA', 'ORY'],\n",
    "    'Hong Kong': ['HKG'],\n",
    "    'Singapore': ['SIN'],\n",
    "    'Shanghai': ['PVG', 'SHA'],\n",
    "    'Dubai': ['DXB'],\n",
    "    'Zürich': ['ZRH'],\n",
    "    'Geneva': ['GVA'],\n",
    "    'Aarhus': ['AAR'],\n",
    "    'Sydney': ['SYD'],\n",
    "    'Wroclaw': ['WRO'],\n",
    "    'Budapest': ['BUD']\n",
    "}\n",
    "'''\n",
    "city_dictionary = {\n",
    "    'Mumbai': ['BOM'],\n",
    "    'London': ['LON'],\n",
    "    'Paris': ['PAR'],\n",
    "    'Hong Kong': ['HKG'],\n",
    "    'Singapore': ['SIN'],\n",
    "    'Shanghai': ['SHA'],\n",
    "    'Dubai': ['DXB'],\n",
    "    'Zürich': ['ZRH'],\n",
    "    'Geneva': ['GVA'],\n",
    "    'Aarhus': ['AAR'],\n",
    "    'Sydney': ['SYD'],\n",
    "    'Wroclaw': ['WRO'],\n",
    "    'Budapest': ['BUD']\n",
    "}\n",
    "\n",
    "\n",
    "#adds corresponding country code to the 'depparture codes'\n",
    "dep_codes = []\n",
    "for city in attendees.keys():\n",
    "    if city in city_dictionary and attendees[city] > 0 :\n",
    "        for i in city_dictionary[city] :\n",
    "            dep_codes.append(i)\n",
    "print(dep_codes)\n",
    "\n",
    "\n",
    "'''\n",
    "#filter relevant airports (ie in dep codes)\n",
    "filter_expression = (\n",
    "    (pl.col(\"DEPCITY\").is_in(dep_codes)) | \n",
    "    (pl.col(\"ARRCITY\").is_in(dep_codes))\n",
    ")\n",
    "'''#taken out because there is a better way\n",
    "'''\n",
    "filtered_schedules = schedules.filter(filter_expression) #gives all rows with airports mentioned in the input\n",
    "filtered_schedules\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b3e5a87e-4a2d-4e3e-9101-25e130cb75fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_753, 11)\n",
      "┌────────┬─────────┬────────┬─────────┬───┬──────────────┬─────────────┬─────────────┬─────────────┐\n",
      "│ DEPAPT ┆ DEPCITY ┆ ARRAPT ┆ ARRCITY ┆ … ┆ LCL_ARR      ┆ TOTAL_SEATS ┆ OAG_SCHEDUL ┆ LCL2UTC     │\n",
      "│ ---    ┆ ---     ┆ ---    ┆ ---     ┆   ┆ ---          ┆ ---         ┆ E_FINGERPRI ┆ ---         │\n",
      "│ str    ┆ str     ┆ str    ┆ str     ┆   ┆ datetime[ms] ┆ i64         ┆ NT          ┆ duration[ms │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ ---         ┆ ]           │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ str         ┆             │\n",
      "╞════════╪═════════╪════════╪═════════╪═══╪══════════════╪═════════════╪═════════════╪═════════════╡\n",
      "│ SYD    ┆ SYD     ┆ MEL    ┆ MEL     ┆ … ┆ 2023-05-12   ┆ 174         ┆ 1b8aa2a1d80 ┆ -10h        │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 11:35:00     ┆             ┆ c48f76b449e ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ f6ae777e…   ┆             │\n",
      "│ SYD    ┆ SYD     ┆ BKK    ┆ BKK     ┆ … ┆ 2023-05-12   ┆ 292         ┆ 1a5326487a7 ┆ -7h         │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 16:20:00     ┆             ┆ b288659496d ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ d686145e…   ┆             │\n",
      "│ BOM    ┆ BOM     ┆ MAA    ┆ MAA     ┆ … ┆ 2023-05-12   ┆ 180         ┆ d008964e185 ┆ -5h -30m    │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 07:30:00     ┆             ┆ ffc4a81bee6 ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ 067416a4…   ┆             │\n",
      "│ HKG    ┆ HKG     ┆ BKK    ┆ BKK     ┆ … ┆ 2023-05-12   ┆ 334         ┆ afa5e73366e ┆ -7h         │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 10:00:00     ┆             ┆ da4f242af41 ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ 163edf85…   ┆             │\n",
      "│ SYD    ┆ SYD     ┆ MEL    ┆ MEL     ┆ … ┆ 2023-05-12   ┆ 174         ┆ 35e5d1a60c5 ┆ -10h        │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 11:35:00     ┆             ┆ f26031a1598 ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ d244c5c4…   ┆             │\n",
      "│ …      ┆ …       ┆ …      ┆ …       ┆ … ┆ …            ┆ …           ┆ …           ┆ …           │\n",
      "│ SYD    ┆ SYD     ┆ AKL    ┆ AKL     ┆ … ┆ 2023-05-13   ┆ 214         ┆ 5fd393de2b9 ┆ -12h        │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 15:05:00     ┆             ┆ 37bea84b65d ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ b32560dd…   ┆             │\n",
      "│ SYD    ┆ SYD     ┆ MCY    ┆ MCY     ┆ … ┆ 2023-05-13   ┆ 110         ┆ 5b9b20e509a ┆ -10h        │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 11:35:00     ┆             ┆ 861d302f3d3 ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ 3cee8219…   ┆             │\n",
      "│ SYD    ┆ SYD     ┆ AKL    ┆ AKL     ┆ … ┆ 2023-05-13   ┆ 214         ┆ 172f5ea3814 ┆ -12h        │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 15:05:00     ┆             ┆ b4eace3d09c ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ 433248bc…   ┆             │\n",
      "│ SYD    ┆ SYD     ┆ AKL    ┆ AKL     ┆ … ┆ 2023-05-13   ┆ 214         ┆ 01a07f798a4 ┆ -12h        │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 15:05:00     ┆             ┆ 6031e36aa01 ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ a5843e47…   ┆             │\n",
      "│ HKG    ┆ HKG     ┆ CAN    ┆ CAN     ┆ … ┆ 2023-05-13   ┆ 262         ┆ a191895afdf ┆ -8h         │\n",
      "│        ┆         ┆        ┆         ┆   ┆ 08:55:00     ┆             ┆ 71794fa23c3 ┆             │\n",
      "│        ┆         ┆        ┆         ┆   ┆              ┆             ┆ 53408466…   ┆             │\n",
      "└────────┴─────────┴────────┴─────────┴───┴──────────────┴─────────────┴─────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(pl.col(\"DEPAPT\").is_in(dep_codes))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c52c55d1-3b21-4a81-8849-1fb413830260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (28, 3)\n",
      "┌─────────┬────────┬──────┐\n",
      "│ ARRCITY ┆ ARRAPT ┆ freq │\n",
      "│ ---     ┆ ---    ┆ ---  │\n",
      "│ str     ┆ str    ┆ u32  │\n",
      "╞═════════╪════════╪══════╡\n",
      "│ HYD     ┆ HYD    ┆ 1    │\n",
      "│ TYO     ┆ HND    ┆ 2    │\n",
      "│ GOI     ┆ GOX    ┆ 2    │\n",
      "│ BOM     ┆ BOM    ┆ 1    │\n",
      "│ ADL     ┆ ADL    ┆ 1    │\n",
      "│ …       ┆ …      ┆ …    │\n",
      "│ TYO     ┆ NRT    ┆ 2    │\n",
      "│ SZX     ┆ SZX    ┆ 2    │\n",
      "│ HKG     ┆ HKG    ┆ 1    │\n",
      "│ MEL     ┆ AVV    ┆ 2    │\n",
      "│ PER     ┆ PER    ┆ 1    │\n",
      "└─────────┴────────┴──────┘\n",
      "['HYD', 'HND', 'GOX', 'BOM', 'ADL', 'SYD', 'GOI', 'SIN', 'CCU', 'SZB', 'MEL', 'CBR', 'KUL', 'ZYK', 'DMK', 'MAA', 'BKK', 'OOL', 'AKL', 'BNE', 'DEL', 'BLR', 'LHR', 'NRT', 'SZX', 'HKG', 'AVV', 'PER']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "targets = []\n",
    "target = df[:, [\"ARRCITY\"]]\n",
    "target = target.with_columns([(pl.col(\"ARRCITY\").count().over(\"ARRCITY\") ).alias(\"frequency\")])\n",
    "target = target.filter(pl.col(\"frequency\")>20).unique()\n",
    "target = target.sort(\"frequency\", descending = True)\n",
    "target = target[:,[\"ARRCITY\"]].rows()\n",
    "for i in dep_codes :\n",
    "    targets.append(i)\n",
    "for i in target :\n",
    "    targets.append(i[0])\n",
    "\n",
    "target_apts = []\n",
    "T_APT = df[:, [\"ARRCITY\",\"ARRAPT\"]]\n",
    "T_APT = T_APT.filter(pl.col(\"ARRCITY\").is_in(targets)).unique()\n",
    "T_APT = T_APT.with_columns([(pl.col(\"ARRCITY\").count().over(\"ARRCITY\") ).alias(\"freq\")])\n",
    "print(T_APT)\n",
    "#################################### THIS SECTION WILL BE NEEDED FOR CONSIDERING ALL AIPORTS AT A MEETING CITY #########################\n",
    "T_APT = T_APT[:,[\"ARRAPT\"]].rows()\n",
    "for i in T_APT :\n",
    "    target_apts.append(i[0])\n",
    "print(target_apts)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "9175211b-f18b-411e-89af-52c9f964960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (49, 3)\n",
      "┌───────────────────┬─────────────────┬─────────────┐\n",
      "│ DEPARTURE_AIRPORT ┆ ARRIVAL_AIRPORT ┆ AV_EMI_SEAT │\n",
      "│ ---               ┆ ---             ┆ ---         │\n",
      "│ str               ┆ str             ┆ f64         │\n",
      "╞═══════════════════╪═════════════════╪═════════════╡\n",
      "│ BOM               ┆ LHR             ┆ 0.659322    │\n",
      "│ SYD               ┆ SZX             ┆ 0.628382    │\n",
      "│ BOM               ┆ SIN             ┆ 0.283874    │\n",
      "│ HKG               ┆ KUL             ┆ 0.188036    │\n",
      "│ BOM               ┆ GOI             ┆ 0.035886    │\n",
      "│ …                 ┆ …               ┆ …           │\n",
      "│ HKG               ┆ LHR             ┆ 0.879551    │\n",
      "│ BOM               ┆ MAA             ┆ 0.061531    │\n",
      "│ SYD               ┆ AVV             ┆ 0.053327    │\n",
      "│ HKG               ┆ SYD             ┆ 0.553867    │\n",
      "│ BOM               ┆ BLR             ┆ 0.051445    │\n",
      "└───────────────────┴─────────────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "df_emissions = pl.read_csv(\"emissions.csv\")\n",
    "\n",
    "df_emissions = df_emissions.filter(pl.col(\"DEPARTURE_AIRPORT\").is_in(dep_codes))\n",
    "df_emissions = df_emissions.drop_nulls(\"ESTIMATED_CO2_TOTAL_TONNES\")\n",
    "df_emissions = df_emissions.filter(pl.col(\"SEATS\")!= 0)\n",
    "df_emissions = df_emissions.sort(\"SEATS\")\n",
    "df_emissions = df_emissions.with_columns([(pl.col(\"ESTIMATED_CO2_TOTAL_TONNES\") / pl.col(\"SEATS\")).alias(\"EMISSIONSpSEAT\")])\n",
    "\n",
    "df_emissions = df_emissions[:,[\"DEPARTURE_AIRPORT\",\n",
    "                               \"ARRIVAL_AIRPORT\",\n",
    "                               \"EMISSIONSpSEAT\",\n",
    "                              # \"SEATS\"\n",
    "                              ]]\n",
    "df_emissions = df_emissions.with_columns([pl.col(\"EMISSIONSpSEAT\").mean().over(\"DEPARTURE_AIRPORT\",\"ARRIVAL_AIRPORT\").alias(\"AV_EMI_SEAT\")])\n",
    "df_emissions = df_emissions[:,[\"DEPARTURE_AIRPORT\",\"ARRIVAL_AIRPORT\",\"AV_EMI_SEAT\"]].unique()\n",
    "df_emissions = df_emissions.filter(pl.col(\"ARRIVAL_AIRPORT\").is_in(target_apts)) \n",
    "\n",
    "print(df_emissions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "0d285f93-c85b-4aea-bb20-2fbf55bf05d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (2891109199.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[365], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    Local_wdt_start = 09:00:00\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "  #---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Event_Length = Event_Dur(json_file_easy)\n",
    "\n",
    "target_score = {}\n",
    "\n",
    "Weight_Vector = [1,1,1]\n",
    "\n",
    "iterating_time = 1  \n",
    "\n",
    "Min_Length_Per_Day = timedelta(hours=1)\n",
    "\n",
    "Local_wdt_start = 09:00:00\n",
    "Local_wdt_end = 17:00:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "ffc2abdc-bcd6-4cd9-8ba8-ea2c6334032c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4130047156.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[366], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    nightlength = (Make_DT(UTC_wdt_start, start_time) - Make_DT(UTC_wdt_end, start_time)) + timedelta(days=1) +\u001b[0m\n\u001b[0m                                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Make_DT(T, DT) :\n",
    "    return datetime.combine(dateTimeInput.date(), timeInput.time())\n",
    "    \n",
    "def End(start_time) :\n",
    "    nightlength = (Make_DT(UTC_wdt_start, start_time) - Make_DT(UTC_wdt_end, start_time)) + timedelta(days=1) +\n",
    "    daylength = Make_DT(UTC_wdt_end, start_time) - Make_DT(UTC_wdt_start, start_time)\n",
    "    nights = (Event_Length//daylength)\n",
    "    return start_time + Event_Length + (nightlength * nights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ad6fd-a821-4a45-a5f5-0bfc1a65764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in targets :   #something to do with if dep and arr being the same gives time and emission as zero\n",
    "\n",
    "#get variables out of polars\n",
    "\n",
    "    \n",
    "    UTC_wdt_start = UTC - LOCAL + Local_wdt_start\n",
    "    UTC_wdt_end = UTC - LOCAL + Local_wdt_end\n",
    "    \n",
    "    #initial start and end\n",
    "    Start_Time = start_date   \n",
    "    End_Time = End(Start_Time)\n",
    "    \n",
    "    Start_Times = []\n",
    "    End_Times = []\n",
    "\n",
    "\n",
    "#------------------ITERATE IN RANGE OF POSIBLE MEETING TIME----------------\n",
    "\n",
    "    while End_Time <= end_date :\n",
    "        good = true\n",
    "        if End_Time < UTC_wdt_start + Min_Length_Per_Day :  \n",
    "            good = false\n",
    "        if Start_Time + Min_Length_Per_Day > Make_DT(UTC_wdt_end, Start_Time) or Start_Time < Make_DT(UTC_wdt_start, Start_Time) :\n",
    "            good = false\n",
    "        if good == true : #read iff start end time -> meeting time per day in range(min,max)\n",
    "            #Start_Times.append(Start_Time)  \n",
    "            #End_Times.append(Start_Time)\n",
    "\n",
    "        #------------------ITERATE WRT ATTENDEES----------------\n",
    "            times = []\n",
    "            emissions = []\n",
    "            \n",
    "            for j in attendees :\n",
    "                emissions.append(find_score(j,target)[1])\n",
    "                times.append(find_score(j,target)[0])\n",
    "                emissions.append(find_score(j,target)[0])\n",
    "                times.append(find_score(j,target)[1])\n",
    "    \n",
    "            T_emission = sum(emissions)\n",
    "            A_time = sum(times)/(len(times))\n",
    "            fairness = np.std(times)\n",
    "    \n",
    "            score = T_emission * Weight_Vector[0] + A_time * Weight_Vector[1] + fairness * Weight_Vector[2]\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "        Start_Time += iterating_time #iterate         #times\n",
    "        End_Time = End(Start_Time)   #next end\n",
    "\n",
    "    \n",
    "   # target_score[target] = [score, time]   #value for specific time   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
